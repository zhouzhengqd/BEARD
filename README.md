<p align="center">
<img src="Fig/logo.png" width="100%" class="center" alt="pipeline"/>
</p>

<div align="center">

<!-- [Zheng Zhou](https://zhouzhengqd.github.io/)<sup>1</sup> &nbsp; [Wenquan Feng](https://shi.buaa.edu.cn/fengwenquan/zh_CN/index/132879/list/)<sup>1</sup> &nbsp; [Shuchang Lyu](https://scholar.google.com/citations?user=SwGcxzMAAAAJ&hl=en)<sup>1</sup> &nbsp; [Guangliang Cheng](https://sites.google.com/view/guangliangcheng)<sup>2</sup> &nbsp; [Xiaowei Huang](https://cgi.csc.liv.ac.uk/~xiaowei/)<sup>2</sup> &nbsp; [Qi Zhao](https://shi.buaa.edu.cn/07297/zh_CN/index.htm)<sup>1</sup>

<sup>1</sup> Beihang Univerisity &nbsp; <sup>2</sup> University of Liverpool -->


</div>

---
<!-- <span class="links">
  <a  href="https://arxiv.org/abs/2411.09265" rel="nofollow"><img src="https://img.shields.io/badge/cs.CV-2411.09265-b31b1b?logo=arxiv&logoColor=red" alt="ArXiv" style="max-width: 100%;"></a>
  <a href="https://beard-leaderboard.github.io/" rel="nofollow"><img alt="Static Badge" src="https://img.shields.io/badge/Website-gray?logo=Google%20chrome&label=Project Page&labelColor=darkorange&logoColor=white" alt="GitHub" style="max-width: 100%;"></a>
  <a href="https://github.com/zhouzhengqd/BEARDt"><img alt="Github" src="https://img.shields.io/badge/Code-gray?logo=Github&label=GitHub&labelColor=black" style="max-width: 100%;"></a>
  <a href="https://share.multcloud.link/share/a51b64d1-063c-4a5c-a7b2-667cf94da71a" rel="nofollow"><img alt="Static Badge" src="https://img.shields.io/badge/Material-gray?logo=icloud&label=Dataset%20Pool&labelColor=orange"></a>
  <a href="https://share.multcloud.link/share/7dd850f1-b263-4f8b-9777-8e3134250187" rel="nofollow"><img alt="Static Badge" src="https://img.shields.io/badge/Materials-gray?logo=icloud&label=Model%20Pool&labelColor=red"></a>
</span> -->

<span class="links">
  <a  href="https://arxiv.org/" rel="nofollow"><img src="https://img.shields.io/badge/cs.CV-anonymous-b31b1b?logo=arxiv&logoColor=red" alt="ArXiv" style="max-width: 100%;"></a>
  <a  href="https://arxiv.org/" rel="nofollow"><img alt= "Static Badge" src="https://img.shields.io/badge/Paper-gray?logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAADIAAAAuCAYAAABqK0pRAAAKAElEQVRo3u1aWW8b1xlNkfahQG0nLlwg6IIW%2FQnuH%2Bjv6PbsN%2F8Bo08BGsBLU%2BexNYq2iW35wW%2FKQ2HXjrWLnBmSM9xFiqS4WItJStRCUvLXc747Q116QUPJdRIgA1zM8M72Leec77sjvfPOt9u329drE5Fzg8HgV%2F3%2B8MN%2Bv%2F%2Fv%2Ff39eYy5%2F8c4ODiwfh%2Fo7z28r1Zb%2BytM%2Be6Xsfc7luHvDYfDD4%2BOjq5hf%2FXw8DCG47685Q3vxHguOzs9SSSTPkz7fmTgu3t7ez%2BDt7%2FgwO%2Bfc48bPsNYhsFL4Ui8%2BNDnz58L5k88aBSfYY9er6fz0dYfDGRzc2v0e2dnB9fsysbGJh1xRo7gxg9wvoybe7zOGq%2FcXnzxacb29rbu7Y1Gn8iRzc3NH%2FP%2Btw0ROvDs2TMejM0D99J%2B1j6xI%2BtfhSPl1ob0Bodj81P37slfbt4cZeVr7whckblSUxIbu2Ozt2%2FfkWvXr785R%2FggyKoOyKxAqWSAYwiBQP50zj7Pee55jse8fjgYKqlf50i60pBGbzA2e%2BfuXblx48abc6SHG2afzMjMkycyPzcnqURCEp4rszOz4jqOzM%2FOytLCoizML0iQ8vV4eXFR3HhcFjE3PzsnM198Ifl84bU5WcikpLHbfQlaN79Z0BL529ID%2BUdyRoZHx1m7OzUlf%2F7442%2BOIyR7o96QDhTKlltm5DqhFUKSMK2v1U%2FvCHnRB%2B6PDo%2F%2Bp2GRlNq1wi56L17f7nRees7tO3fk6rVrI%2Bd2d3elXF49vSOrpZL4yaS0251Xnl%2BrVCTjB7JSKEoRfPATSSkXV%2FS4kM3inC%2FZIC2ZVEqviQykI81mU3a622OCQGjd%2FOST0XV7J3GE0acRNRjXQDrXKlVZq%2FJ4DQ6VcVzDuVWkeg37qipTEca6yzGIQVISjivxpWXdJ3kM8jtLS%2BJ7niQgApynskUbBcTBvft7%2B6O5z27fHpPfvd09qeBdEznSqtf1hV4sLkEihegmNMJpRFMNiTuS8hL6O5%2FJquOrKyuYdzHnm%2FuSqdE9Pq51YzEp5PL6LF7T7x874uJ5HgaNHWXk7pRcvXp1DFrVSm0yR%2Bq1mr5MjcILGEkaQ1h4MMiDMw4i7iGKNHYXDywXCuJizofjSc2EI0nXU7glHU%2BfxWuZNTrJLEZbNpORJghv8%2Bdfn34qf%2Froo9NxhHgPwAcaTQOyQaCZyKbTmgkaWsjlJIffHP0Dk5GU6yqEUq7JVnxxCRkl1Bx9Du%2F1vaQeR3yg8a3WU9lB43hkcWR6%2BnOZmro3cgTrD2k0mpM50mBGksYADsLHXV4OjzNKWjpAmKQQdVbxarkM%2BMTVSe7TIHcSjtFwOkg4MosRLG2O%2BCykKJxs3aNtfX1jTEz2wB82khM7QgMZOcKDkGBklSswIpdJq8E0iJlje1IpGUey6axmw0DKHTnHZzCLhFALKmXXjET4Lps3efCpZpFboVWaEFp1qJLiGlhOqSMGTjSOGSHMdA6RJuzoSHtrSyNeTAdw0FO5zYSw5D3kBWW4BOmtAuuHwxBaGGlcYxzpjzmSSQWj7h6LPqlV1yYle1XhQJJn8JIgzIiSV7HuKf7VIWTlAPhdyefVEc57IXx4Xnmj1zo6oiwPwuiTIzSaQWP1HsENmc4AwpEAnKiO1KtVzURGlSulL1dsY87IakKdoaMGWgMpg%2BxUJAMjUxeU4K5nnMB81k9rQNLIrF38kuBRFJBoq6MEUACOObI3ufyy8KliwWhGj8ZEXKGRAWuIws5VR0n2Cio%2FocdhMrYMfjh6DdXO1ww5%2BgwWTZsPHrkEx21HSHY%2Bd8yRam3SgtiQfDqjLye%2BTVEzhEzhpSx4EUx4jvJL3LOuqDqxwuNeXsvaQ3hqMPCbGaWzdh3xwEfCct9yJAc%2B8doIWiciexMZSVCBgFHKrk3eSL0IKdYLSvMBsM3ao7DyonO4B4YUcZ7ZZIdAyWam%2BZzh8BhaTsw4b2eAvEkBcnZGVlcrkzmytbGh%2FVUTOGWzuI6CRXIy8uyAo9VgtGfUWEeYDWbL0Qqf1AxFnGLFZ5YiHkV1hPcWoGQ18HJgwS0DGSfZTwUtpp03RuPF1vtVWylsURj9hGsIz8hTxglPSjWdYj9Gp2xHctmc8shWLTaSrEnHqoWMlCfMyHa3i4hktOJS4zuddrj2HowGDTkM51jcWB9IbsKREWd2suACRYE84lzUv%2BXw7HGOuApFmyM%2BeZk45ghhN3GLQtnLM7UwpLxS0pcXtUD5JqqOUSIaSaMYyRWsO5Jh8eR50zUftzmEnPZZoRLyg8TIEa0x7hi0suAW1y12QaxWq5PKb00jSkciY0hsEl4rfbiucMKeSqMFgXAoDFo44UgqoU6bOpRSkWAg3OW4NBuNsRblVfL74pfGE5E9Wu0Fox7LqBCjTwXjcQERY3SDsCK3kHYaQ5Wj5JYKeYVbLmw%2FcuzBoGY53Ef42dBiQUxq89m3CmJDv%2BCMye%2BkHKECcYWXCfFuSOtqtKNGUesB5ukwi1sphFYqXIOktV4QkjnTTAbpkeO6HhnYjiSkgjpkd8ROLBbWkTAj%2B7tSqa5OLr%2BmFrhh45cO60BGI7%2BJmzfW1%2FW6TexpQLlY1FWgj5dzT55wAWZqUUyW5xcNb3RlGYyp1tOnL38qoMgwaFFGuAxm2zKxI37Y9JGkhAOrNAtaNmwiWSuYJf7mS7iWZ8T9MFtOWP1NXYlJsZCRUhXdby0nZYzBcGAtrFq6yrRlnv0Yl8a2%2FJ6oaYzW6TTGtBaGLwotQi0eD9sNX7G9Wlwx58JrymgxVDDYcEIslPi4lo5yXWITmS2MB%2Bm22%2FgglGqbI6snWepG3S4HW3MqD2ESW1jS4YVtOSOuioLud3WlrM2jccCsZeiYp%2BsXo1q694MxPiSpdAiWXUfIRyLBzsjElb0OR%2FywtfZ0TRGuP8IPCrGFRXWKqsTFEslu1vmmnyLEKBaUWha1IAAEs0G4RnFDslvdL%2BbieKb9OaiITqFeq4%2F9fWRjY8JPpr2dbf2uRTiZFZ4pjnkYzcJH%2FLaaLfRgLbxsTSHBjJg236xfeE8a6w8SO0iYjxHRh4tMgMpufXxgxW5vPRuT5An%2FYuW%2B9ksjH8IWhBBgxY1alGiuH35CZTHUvwVGLUx4Pvr7oN43HI4d252v%2FsWq3X5JtWig7dgAx93u8ULLNKuic3HHcd%2FIR%2BzDw%2BGpPmJ3ut2X5jud8TlmMJ8vVre2tgqwtZjNZpcfPPjP32%2FduvXHK1eu%2FBouvBs58hNc3%2F4q%2FqzQbLW6UKUmRosD4tFaWFj45%2BPHj3%2Fz6NGj3z98%2BPAP09PTv7148eIvz549e%2F7MmTM%2FhMlnX%2FmH9Gaz%2BaNer%2Feo0%2Bn43W439bYG33f%2F%2Fv3fXb58%2BYNLly79NBrnz7%2FG0C%2FzTwEXLlz4wblz597HeO8tjvfx7u%2B9qf%2FE%2BC%2FkEJZILUQcyQAAAABJRU5ErkJggg%3D%3D&label=Coming%20Soon&labelColor=367DBD" style="max-width: 100%;"></a>
  <a href="https://beard-leaderboard.github.io/" rel="nofollow"><img alt="Static Badge" src="https://img.shields.io/badge/Website-gray?logo=google%20chrome&label=Project%20Page&labelColor=darkorange&logoColor=white" style="max-width: 100%;"></a>
  <a href="https://github.com/"><img alt="Github" src="https://img.shields.io/badge/Code-gray?logo=Github&label=GitHub&labelColor=black" style="max-width: 100%;"></a>
  <a href="https://share.multcloud.link/share/a51b64d1-063c-4a5c-a7b2-667cf94da71a" rel="nofollow"><img alt="Static Badge" src="https://img.shields.io/badge/Material-gray?logo=icloud&label=Dataset%20Pool&labelColor=orange"></a>
  <a href="https://share.multcloud.link/share/7dd850f1-b263-4f8b-9777-8e3134250187" rel="nofollow"><img alt="Static Badge" src="https://img.shields.io/badge/Materials-gray?logo=icloud&label=Model%20Pool&labelColor=red"></a>
</span>

*Welcome to the official PyTorch implementation of [BEARD: Benchmarking the Adversarial Robustness for Dataset Distillation](https://arxiv.org/).*

BEARD is an open-source benchmark specifically designed to evaluate and improve the **adversarial robustness** of Dataset Distillation (DD) methods. It provides a comprehensive assessment across three key stages: **distillation**, **training**, and **evaluation**.

<p align="center">
<img src="Fig/intro.png" width="100%" class="center" alt="pipeline"/>
</p>

üîπ Explore  the **official leaderboard** here: **[BEARD Leaderboard](https://beard-leaderboard.github.io/)**

<!-- **‚ùóNote‚ùó: If you encounter any issues, please feel free to contact us via email: zhengzhou@buaa.edu.cn.** -->

**‚ùóNote‚ùó: If you encounter any issues, please feel free to contact us via email**

## üöÄ What's New?  

- **Mar. 2025**: We have updated our attack library with **transfer-based** and **query-based black-box attacks** along with their evaluation files.  
- **Sep. 2024**: The full BEARD codebase is now open-source! üéâ Access it here: [BEARD GitHub Repository](https://github.com).  
- **Aug. 2024**: The first full release of the BEARD benchmark project.  

## üéØ Overview of BEARD

<p align="center">
<img src="Fig/overview.png" width="100%" class="center" alt="pipeline"/>
</p>

BEARD addresses a critical gap in **dataset distillation** research by providing a systematic framework for evaluating adversarial robustness. While significant progress has been made in DD, deep learning models trained on distilled datasets remain vulnerable to adversarial attacks, posing risks in real-world applications.

### üî• Key Features:
- **Unified Benchmark**: Evaluate DD methods across multiple datasets and attack scenarios.
- **New Evaluation Metrics**: Includes the **Robustness Ratio (RR)**, **Attack Efficiency Ratio (AE)**, and **Comprehensive Robustness-Efficiency Index (CREI)**.
- **Open-Source Tools**: Easily integrate and evaluate the robustness of your DD methods with BEARD's extensible framework.

## üõ† Getting Started
Follow the steps below to set up the environment and run the BEARD benchmark.

### Step 1: Clone the Repository
- Run the following command to download the Repo.
  ```
  git clone https://github.com/zhouzhengqd/BEARD.git
  ```
### Step 2: Download Dataset and Model Pools
- Download the required files and place them in the appropriate directories: [Data](https://share.multcloud.link/share/bbe57236-3ca2-42b2-aa10-88394c2c4b04) | [Dataset Pool](https://share.multcloud.link/share/a51b64d1-063c-4a5c-a7b2-667cf94da71a) | [Model Pool](https://share.multcloud.link/share/7dd850f1-b263-4f8b-9777-8e3134250187).
<!-- - ‚ùó**Alternative Access**: If you are unable to access the **MultCloud** drive, you can find the files on **Google Drive**: [Data](https://drive.google.com/drive/folders/1ntfXLEFHRPFPC3JG_hFOkcrEh5BKSzsp?usp=drive_link) | [Dataset Pool](https://drive.google.com/drive/folders/1gtgmGEM7zZXG0al-iJQsxxhTiOD5SqLG?usp=drive_link) | [Model Pool](https://drive.google.com/drive/folders/1tFK0GTWNrFp0Vu-L_lN1WXGndiPnldZq?usp=drive_link). -->

### Step 3: Set Up the Conda Environment
- Run the following command to create a conda environment
    ```
    cd BEARD
    cd Code
    conda env create -f environment.yml
    conda activate beard
    ```
## üìÅ Directory Structure
- `BACON`
    - `Code`
        - `data`
          - `datasets`
        - `dataset_pool`
        - `model_pool`
        - `evaluate_model.py`
        - `train_model.py`
        - `evaluate_model_blackbox.py`
        - `evaluate_config.json`
        - `train_config.json`
        - `evaluate_config_blackbox.json`
        - Files for BEARD
        - `enviroment.yml`
        - ...
        - ...
        - ...
## üö¶ Quick Evaluation Command
### Step 1: Download Dataset and Model Pools
- Ensure you have downloaded the dataset and model pools from the links provided above.
### Step 2: Modify Evaluation Configuration
- Adjust the evaluation configuration by editing the `evaluate_config.json` file based on your requirements.
### Step 3: Run the Evaluation Script
- Execute the evaluation to assess adversarial robustness:
  ```
    python evaluate_model.py --config ./evaluate_config.json
  ```
- To evaluate **transfer-based black-box attacks**, use:
  ```
    python evaluate_config_blackbox.py --config ./evaluate_config_blackbox.json
  ```
- **Note:** If your model was trained using **distributed training**, ensure that you also use the corresponding **distributed evaluation** setup for consistency. For instance, **IDM** in the model pool is trained with distributed training, so we provide a **single-GPU version** in the model pool for evaluation.
## ‚ûï Adding New Datasets and Models
### Step 1: Add Datasets
- Place the newly generated distilled datasets in the `dataset_pool` directory.
### Step 2: Modify Training Configuration
- Adjust the training configuration by editing the `train_config.json` file to specify the new datasets.
### Step 3: Run the Training Script
- Train the models on the new datasets:
  ```
    python train_model.py --config ./train_config.json
  ```
### Step 4: Evaluate the Models
- Once the models are trained, follow the evaluation steps outlined in the "Quick Evaluation Command" section to evaluate adversarial robustness.

## üåê Join the Community
If you're working on DD or adversarial robustness, we invite you to contribute to the BEARD benchmark, explore the leaderboard, and share your insights.

## üôè Acknowledgments

We would like to thank the contributors of the following projects that inspired and supported this work:

- [DC, DSA, DM](https://github.com/VICO-UoE/DatasetCondensation)
- [MTT](https://github.com/GeorgeCazenavette/mtt-distillation)
- [IDM](https://github.com/uitrbn/IDM)
- [BACON](https://github.com/zhouzhengqd/BACON)
- [torchattacks](https://github.com/Harry24k/adversarial-attacks-pytorch)

<!-- ## üìö Citation
```
@inproceedings{zhou2025beard,
  title={BEARD: Benchmarking the Adversarial Robustness for Dataset Distillation},
  author={Zhou, Zheng and Feng, Wenquan and Lyu, Shuchang and Cheng, Guangliang and Huang, Xiaowei and Zhao, Qi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025}
}
```

## üåü Star History

[![Star History Chart](https://api.star-history.com/svg?repos=zhouzhengqd/BEARD&type=Date)](https://star-history.com/#zhouzhengqd/BEARD&Date) -->